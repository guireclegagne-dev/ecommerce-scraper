# ğŸ›’ E-Commerce Scraper - Guide d'Installation et d'Utilisation

## ğŸ“‹ Table des matiÃ¨res
1. [Installation](#installation)
2. [Configuration initiale](#configuration-initiale)
3. [Utilisation](#utilisation)
4. [Configuration avancÃ©e](#configuration-avancÃ©e)
5. [RÃ©solution des problÃ¨mes](#rÃ©solution-des-problÃ¨mes)

---

## ğŸš€ Installation

### PrÃ©requis
- Python 3.8 ou supÃ©rieur
- pip (gestionnaire de packages Python)
- Chrome ou Chromium (pour Selenium)

### Ã‰tape 1 : Installation des dÃ©pendances

```bash
# Installer toutes les dÃ©pendances
pip install -r requirements.txt

# OU installer manuellement
pip install streamlit requests beautifulsoup4 selenium APScheduler supabase pandas openpyxl
```

### Ã‰tape 2 : Installation du driver Chrome (pour Selenium)

```bash
# Le driver sera installÃ© automatiquement au premier lancement
# Ou installez-le manuellement :
pip install webdriver-manager
```

### Ã‰tape 3 : Lancer l'application

```bash
streamlit run app.py
```

L'application sera accessible Ã  l'adresse : **http://localhost:8501**

---

## âš™ï¸ Configuration initiale

### 1ï¸âƒ£ Configuration de Supabase

#### Option A : Via l'interface web
1. Allez dans **Configuration > Base de donnÃ©es**
2. SÃ©lectionnez "supabase"
3. Entrez votre **URL du projet** (https://xxxxx.supabase.co)
4. Entrez votre **clÃ© API** (anon key)
5. Cliquez sur **Sauvegarder**

#### Option B : CrÃ©er les tables dans Supabase
1. Connectez-vous Ã  votre projet Supabase
2. Allez dans **SQL Editor**
3. ExÃ©cutez le script suivant :

```sql
CREATE TABLE IF NOT EXISTS produits (
    id BIGSERIAL PRIMARY KEY,
    marque TEXT,
    modele TEXT NOT NULL,
    finitions TEXT,
    caracteristiques TEXT,
    prix TEXT,
    url TEXT,
    image TEXT,
    disponibilite TEXT,
    site_source TEXT,
    date_collecte TIMESTAMP DEFAULT NOW(),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Index pour amÃ©liorer les performances
CREATE INDEX IF NOT EXISTS idx_produits_marque ON produits(marque);
CREATE INDEX IF NOT EXISTS idx_produits_modele ON produits(modele);
CREATE INDEX IF NOT EXISTS idx_produits_site ON produits(site_source);
CREATE INDEX IF NOT EXISTS idx_produits_date ON produits(date_collecte);

-- Trigger pour mettre Ã  jour updated_at automatiquement
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_produits_updated_at BEFORE UPDATE ON produits
FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
```

### 2ï¸âƒ£ Ajouter votre premier site

1. Allez dans **Gestion des Sites > Ajouter un site**
2. Remplissez les informations :
   - **Nom du site** : Ex: "Amazon France"
   - **URL du catalogue** : L'URL de la page produits
   - **Type** : E-commerce / Marketplace
   - **Authentification** : Cochez si le site nÃ©cessite un login
3. Cliquez sur **Ajouter le site**

### 3ï¸âƒ£ Configurer l'authentification (si nÃ©cessaire)

Si un site nÃ©cessite une authentification :

1. Allez dans **Configuration > Identifiants**
2. SÃ©lectionnez le site
3. Entrez vos identifiants
4. Sauvegardez

âš ï¸ **Important** : Les identifiants sont stockÃ©s localement dans `data/credentials/`. Assurez-vous de protÃ©ger ce dossier.

### 4ï¸âƒ£ Activer la collecte automatique

1. Allez dans **Configuration > Planification**
2. Cochez **Activer la collecte automatique quotidienne**
3. Choisissez l'heure (par dÃ©faut 09:00)
4. Sauvegardez

---

## ğŸ“– Utilisation

### Dashboard
Le dashboard affiche :
- Nombre de sites surveillÃ©s
- Sites actifs
- Produits collectÃ©s
- DerniÃ¨re collecte
- ActivitÃ© rÃ©cente

### Lancer une collecte manuelle
1. Depuis le **Dashboard**
2. Cliquez sur **ğŸš€ Lancer une collecte maintenant**
3. Attendez la fin du processus

### GÃ©rer les sites

#### Ajouter un site
- **Gestion des Sites > Ajouter un site**
- Remplissez le formulaire
- Les sÃ©lecteurs CSS sont optionnels (dÃ©tection automatique)

#### Modifier un site
- **Gestion des Sites > Liste des sites**
- DÃ©veloppez le site Ã  modifier
- Ajustez les paramÃ¨tres

#### DÃ©sactiver/Supprimer un site
- **Gestion des Sites > Liste des sites**
- Cliquez sur **DÃ©sactiver** ou **Supprimer**

### Exporter les donnÃ©es

1. Allez dans **Export**
2. Choisissez le format (CSV, Excel, JSON)
3. SÃ©lectionnez les sites et champs
4. Cliquez sur **Exporter**

Les fichiers sont sauvegardÃ©s dans `data/exports/`

### Consulter les logs

1. Allez dans **Logs**
2. Filtrez par date, site ou statut
3. Consultez les dÃ©tails de chaque collecte

---

## ğŸ”§ Configuration avancÃ©e

### SÃ©lecteurs CSS personnalisÃ©s

Pour une meilleure prÃ©cision, vous pouvez dÃ©finir des sÃ©lecteurs CSS :

```
Marque : .product-brand, [data-brand]
ModÃ¨le : .product-title, h2.name
Finitions : .variant-selector, .color-option
CaractÃ©ristiques : .specs ul, .features
```

**Astuce** : Utilisez les outils de dÃ©veloppement de votre navigateur (F12) pour identifier les sÃ©lecteurs.

### Utiliser une autre base de donnÃ©es

#### SQLite (local)
```python
# Dans Configuration > Base de donnÃ©es
Type : sqlite
Chemin : data/scraper.db
```

#### PostgreSQL
```python
# Dans Configuration > Base de donnÃ©es
Type : postgresql
HÃ´te : localhost
Port : 5432
Base : scraper_db
Utilisateur : postgres
Mot de passe : ****
```

### Ajuster le nombre de pages scrapÃ©es

Ã‰ditez `scheduler.py` ligne 67 :
```python
products = scraper.scrape_multiple_pages(
    site['url'],
    max_pages=10,  # Changez cette valeur
    selectors=selectors
)
```

### Mode headless (sans interface Chrome)

Le mode headless est activÃ© par dÃ©faut. Pour le dÃ©sactiver :

Ã‰ditez `scraper.py` ligne 34 :
```python
chrome_options.add_argument('--headless')  # Commentez cette ligne
```

---

## ğŸ› RÃ©solution des problÃ¨mes

### Erreur : "Module not found"
```bash
# RÃ©installez les dÃ©pendances
pip install -r requirements.txt --force-reinstall
```

### Erreur : "Chrome driver not found"
```bash
# Installez le gestionnaire de driver
pip install webdriver-manager
```

### Erreur de connexion Supabase
- VÃ©rifiez votre URL et clÃ© API
- VÃ©rifiez que les tables sont crÃ©Ã©es
- VÃ©rifiez votre connexion internet

### Aucun produit collectÃ©
1. VÃ©rifiez l'URL du site (doit pointer vers une page de catalogue)
2. Le site utilise peut-Ãªtre du JavaScript â†’ Activez Selenium
3. Ajoutez des sÃ©lecteurs CSS personnalisÃ©s
4. Consultez les logs pour plus de dÃ©tails

### Authentification Ã©choue
1. VÃ©rifiez les identifiants
2. Le site utilise peut-Ãªtre un CAPTCHA
3. Essayez d'augmenter les temps d'attente dans `scraper.py`

### La collecte automatique ne se lance pas
1. VÃ©rifiez que la planification est activÃ©e
2. VÃ©rifiez l'heure configurÃ©e
3. RedÃ©marrez l'application

---

## ğŸ“ Structure des fichiers

```
projet/
â”œâ”€â”€ app.py                 # Application principale Streamlit
â”œâ”€â”€ scraper.py            # Module de scraping
â”œâ”€â”€ database.py           # Gestion des bases de donnÃ©es
â”œâ”€â”€ scheduler.py          # Planification automatique
â”œâ”€â”€ exporter.py           # Export des donnÃ©es
â”œâ”€â”€ requirements.txt      # DÃ©pendances
â”œâ”€â”€ README.md            # Ce fichier
â””â”€â”€ data/                # DonnÃ©es de l'application
    â”œâ”€â”€ config.json      # Configuration
    â”œâ”€â”€ sites.json       # Liste des sites
    â”œâ”€â”€ credentials/     # Identifiants (sÃ©curisÃ©s)
    â”œâ”€â”€ logs/           # Logs des collectes
    â””â”€â”€ exports/        # Fichiers exportÃ©s
```

---

## ğŸ” SÃ©curitÃ©

âš ï¸ **Recommandations importantes** :

1. **Ne partagez jamais vos identifiants** stockÃ©s dans `data/credentials/`
2. **ProtÃ©gez votre clÃ© API Supabase** (ne la commitez pas dans Git)
3. **Utilisez un fichier .env** pour les variables sensibles
4. **Respectez les robots.txt** des sites scrapÃ©s
5. **Ajoutez des dÃ©lais** entre les requÃªtes pour ne pas surcharger les serveurs

---

## ğŸ“Š Performances

### Optimisations
- **Mode requests** : Plus rapide, sans JavaScript (par dÃ©faut)
- **Mode Selenium** : Plus lent, avec JavaScript (pour sites complexes)
- **Pagination** : LimitÃ©e Ã  5 pages par dÃ©faut (configurable)
- **DÃ©lais** : 2 secondes entre chaque page

### CapacitÃ©s
- **Sites surveillÃ©s** : IllimitÃ©
- **Produits** : Des milliers par collecte
- **FrÃ©quence** : Quotidienne recommandÃ©e

---

## ğŸ†˜ Support

Pour toute question ou problÃ¨me :
1. Consultez les logs dans l'application
2. VÃ©rifiez la documentation de chaque module
3. Testez avec un seul site d'abord

---

## ğŸ“ Notes importantes

### LÃ©galitÃ© du scraping
- Respectez les conditions d'utilisation des sites
- Consultez les fichiers robots.txt
- N'utilisez pas cet outil Ã  des fins commerciales sans autorisation
- Respectez les limites de taux (rate limits)

### Bonnes pratiques
- Testez toujours sur un petit Ã©chantillon d'abord
- Surveillez les logs pour dÃ©tecter les erreurs
- Mettez Ã  jour les sÃ©lecteurs si les sites changent
- Faites des sauvegardes rÃ©guliÃ¨res de votre base de donnÃ©es

---

## ğŸ¯ FonctionnalitÃ©s futures

AmÃ©liorations possibles :
- âœ… Export automatique vers Google Drive
- âœ… Notifications par email
- âœ… DÃ©tection automatique des changements de prix
- âœ… Interface de comparaison de prix
- âœ… API REST pour accÃ¨s externe
- âœ… Dashboard de visualisation avancÃ©

---

**Version** : 1.0
**Date** : Janvier 2025
